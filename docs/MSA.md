플랫폼에 통합된 모든 데이터 자산 위에 실제 세계의 객체와 개념들을 연결하는 **디지털 트윈** 역할을 합니다  . 이 Ontology 백엔드는 여러 **마이크로서비스**로 구성되어 객체(Object) 데이터를 **인덱싱**, **저장**, **질의** 및 **조작**하는 기능을 분담합니다  . Ontology를 구성하는 주요 마이크로서비스에는 다음이 포함됩니다 :

- **Ontology Metadata Service (OMS)** – 온톨로지 메타데이터 서비스
- **Object Storage V2 (OSv2)** – 객체 데이터 저장소 (차세대 객체 DB)
- **Object Set Service (OSS)** – 객체 집합 서비스 (조회 서비스)
- **Actions Service** – 액션 서비스 (사용자 편집 처리)
- **Object Data Funnel** – 데이터 펀넬 (데이터 쓰기 파이프라인)
- **Functions on Objects** – 객체 함수 서비스

이들 서비스 각각은 독립적인 마이크로서비스로 배포되어 특정 역할과 API를 담당하며, 상호 결합을 낮추기 위해 명확한 경계를 갖습니다. 아래에서는 각 서비스의 기능과 **API 분리 목적**, 서비스 간 **호출 구조와 데이터 흐름**, 그리고 **운영상의 특성**(배포 및 스케일링, 장애 격리와 복구, 모니터링 등)을 상세히 설명합니다.

## **Ontology Metadata Service (OMS)**

- *온톨로지 메타데이터 서비스(OMS)**는 Ontology에 존재하는 모든 **온톨로지 엔티티**의 정의를 총괄하는 서비스입니다. 즉, 어떤 **객체 타입(Object Type)**이 있고 그 속성들은 무엇인지, 객체 간의 **링크 타입(Link Type)**은 어떻게 연결되는지, 또 객체 데이터를 변경할 수 있는 **액션 타입(Action Type)**은 무엇인지 등의 **스키마 메타데이터**를 관리합니다 . OMS는 Ontology의 **스키마 정의와 관리** 전반을 맡고 있어, 다른 서비스들이 데이터 작업을 수행할 때 참조하는 **공통 메타데이터 저장소** 역할을 합니다. 이를 통해 데이터 저장/질의 로직을 메타데이터 관리로부터 **분리**하여, 스키마 변경이 있을 때에도 데이터 인덱싱/질의 서비스에 최소한의 영향만 주도록 합니다. 예를 들어, 새로운 객체 타입을 정의하거나 속성을 변경하면 OMS에서 이를 일관되게 저장하고, 다른 서비스 (예: Funnel)에 **이벤트**를 전파하여 해당 변경에 따른 인덱싱 파이프라인 재구성을 트리거합니다 (메타데이터 변경이 데이터 인덱싱에 반영되도록 이벤트를 발행함) .

**운영 측면**에서 OMS는 **독립 배포되는 마이크로서비스**이며, 주로 메타데이터 CRUD 요청을 처리합니다. 비교적 **부하가 낮은** 서비스이므로 여러 인스턴스로 **수평 확장(scale-out)**하더라도 트래픽이 폭증하는 경우는 드뭅니다. OMS는 내부적으로 메타데이터를 위한 **영구 저장소**(예: Terminus DB)를 가지고 있어, 서비스 재시작이나 장애 시에도 스키마 정의를 복구할 수 있습니다. OMS가 중단되면 새로운 객체 정의나 스키마 변경은 일시 지연되지만, **이미 정의된** 객체 데이터의 조회/인덱싱은 **영향을 받지 않도록** 설계됩니다. 이를 위해 다른 서비스들은 필요한 메타데이터를 **캐시**하거나 OMS 응답 실패 시 **서킷 브레이커**를 통해 빠르게 실패를 감지하고 기본 동작을 유지합니다. OMS 복구 시 대기 중이던 메타데이터 변경 이벤트를 다시 전달하여 **정합성**을 맞추도록 구성됩니다. 요약하면, OMS는 **스키마 레지스트리** 역할을 하며, **API 레벨에서 스키마와 데이터 기능을 분리**함으로써 다른 서비스의 안정성과 독립성을 높입니다.

## **Object Storage V2 (OSv2) – 객체 저장소 (객체 DB)**

- *Object Storage V2 (OSv2)**는 **객체 데이터 저장소**로서, Ontology 내 모든 **객체 인스턴스 데이터**를 보관하는 **전용 데이터베이스 서비스들**의 집합입니다  다시 말해, OSv2에서는 **인덱싱** 담당 부분과 **질의** 담당 부분이 분리되어 각각 독립적으로 확장/운영될 수 있습니다 .

OSv2의 핵심 역할은 **객체 데이터의 저장 및 고속 조회**입니다. 여기에는 Foundry의 다양한 데이터 소스(데이터셋 등)로부터 **통합/인덱싱된 객체 데이터**를 보관하고, 사용자 애플리케이션의 검색, 필터, 집계 요청에 응답할 수 있도록 **최적화된 쿼리 엔진**을 제공하는 것이 포함됩니다 . OSv2는 사용 사례에 따라 서로 다른 특성을 가진 **전문화된 데이터베이스**들을 활용하는데, 예를 들어 **검색**에 특화된 인덱스, **대규모 집계**에 적합한 분산 쿼리 엔진 등이 조합되어 동작합니다. 실제로 OSv2에서는 대규모 객체 집합에 대한 고급 질의를 위해 **Spark 기반의 분산 쿼리 레이어**도 활용되며, 이를 통해 수십만 객체에 대한 검색이나 정확한 집계를 지원합니다 . 이러한 구조 덕분에, OSv2는 **인덱싱 작업과 질의 작업의 부하를 서로 격리**하여 각각 독립적으로 확장할 수 있고, 특정 쿼리 패턴이나 데이터 규모에 최적화된 **데이터 저장 기법**을 적용할 수 있습니다  .

**API 분리 목적**으로 보면, OSv2 자체는 **저수준의 데이터 저장/검색 기능**을 갖지만 직접 외부에 노출되지 않고, 상위에 **전용 읽기 서비스(OSS)**와 **쓰기 파이프라인(Funnel)**을 두어 접근합니다. 즉, 외부 서비스나 UI는 OSv2의 내부 세부 구현을 몰라도, OSS의 읽기 API나 Actions/펀넬의 쓰기 API를 통해 간접적으로 객체 데이터를 다루게 됩니다. 이렇게 함으로써 OSv2 내부를 변경하거나 새로운 저장 기술을 도입해도 외부에는 영향이 최소화되고, **보안과 권한 제어**도 중앙에서 관리할 수 있습니다 .

**운영 특성**으로, OSv2는 **상태ful한 분산 저장소**로서 다수의 노드나 인스턴스로 구성될 수 있습니다. 예를 들어 객체 데이터를 **샤딩**하거나 **복제**하여 대용량 데이터를 처리하고 고가용성을 확보합니다. OSv2의 인덱스는 **에페메랄(ephemeral)** 성격으로 간주되어, 모든 객체 데이터는 결국 원본 데이터소스와 펀넬이 관리하는 영구 데이터셋에 의해 복구될 수 있게 설계되었습니다 . 즉, OSv2에 저장된 인덱싱 데이터는 **캐시 및 성능 향상용**으로, 만약 OSv2에 장애가 발생하면 **원본 데이터와 사용자 편집 로그를 재인덱싱**하여 복원할 수 있습니다. 이처럼 **데이터 영속성을 외부에 위임**함으로써, OSv2 클러스터 자체는 성능에 집중하고 복잡한 백업/복구 부담을 줄였습니다. OSv2 서비스는 높은 처리량과 낮은 지연을 위해 내부에서 **병렬 처리**와 **메모리 기반 인덱스** 등을 활용하며, 부하 증가 시 노드를 추가해 **수평 확장**합니다. 또한, OSv2 노드 간 또는 OSv2와 다른 서비스 간 호출에서는 **타임아웃**과 **서킷 브레이커** 전략을 사용해 문제가 있는 노드에 대한 호출을 차단하고 빠르게 실패시킴으로써 전체 시스템의 **탄력성**을 높입니다. 장애가 발생한 노드는 Apollo/Skylab 등의 배포 인프라에 의해 자동으로 재기동되거나 교체되며, 필요 시 해당 샤드의 데이터를 재색인해서 일관성을 회복합니다.

## **Object Set Service (OSS)**

- *객체 집합 서비스(OSS)**는 Ontology에서 **데이터 조회(read)**를 담당하는 전용 서비스입니다. 다른 Foundry 애플리케이션이나 서비스가 **객체를 검색, 필터링, 집계, 로드**하고자 할 때, 직접 OSv2 DB에 질의하는 대신 OSS의 API를 통해 요청을 합니다 . OSS는 요청을 받아 내부적으로 **객체 데이터베이스**(OSv2의 쿼리 레이어)에 질의를 수행하고 결과를 조합하여 응답합니다. 이렇게 읽기 경로를 전담하는 서비스로 OSS를 둠으로써 **읽기 API**를 일원화하고, 쿼리 최적화나 권한 필터링, 캐싱 등을 한 곳에서 처리할 수 있습니다. 예를 들어, OSS는 쿼리시 **사용자 권한에 따른 필터링**을 적용하고, **검색어**나 **필드 필터**를 해석하여 해당 OSv2 인덱스로 효율적으로 전달합니다. 또한 검색 결과로 나온 객체 모음을 **Object Set**이라는 형태로 캡슐화하여 다른 서비스에 전달하거나 공유할 수 있게 합니다 . Object Set은 특정 조건으로 검색된 객체들의 리스트로, **정적(static)**이거나 **동적(dynamic)** 정의를 가질 수 있으며, 일시적으로 유지되거나 영구 저장될 수도 있습니다 . OSS는 이러한 **객체 집합 관리 기능**도 제공하여, 예컨대 사용자가 대시보드에서 필터를 적용해 얻은 객체 리스트를 임시 Object Set으로 전달하고, 다른 모듈에서 이를 받아 활용하는 시나리오를 지원합니다.

**API 분리 및 경계** 측면에서 OSS는 **데이터 조회 전용 API 계층**으로, 저장 계층(OSv2)의 복잡도를 숨기고 클라이언트에게 **간결하고 의미 있는 쿼리 인터페이스**를 제공합니다. 이는 웹 애플리케이션이나 외부 시스템이 OSS를 통해 객체를 질의할 때, SQL이나 저수준 인덱스 질의 대신 **비즈니스 도메인 용어**(객체 타입명, 속성명 등)로 질의할 수 있도록 해줍니다. 또한 OSS가 중앙 허브 역할을 함으로써, 대량 질의로 인한 부하나 비정상적 접근을 제어하고, 필요 시 캐싱이나 **스파크 분산 쿼리**를 통한 **대규모 연산**도 투명하게 처리해줄 수 있습니다 .

**운영 특성**을 보면, OSS는 **상태가 없는(stateless)** 애플리케이션 서비스로 구현되어 수평 확장이 용이합니다. 다수의 OSS 인스턴스를 부하분산시켜 동시 사용자 질의를 처리할 수 있으며, 인스턴스들은 요청을 받을 때마다 OSv2에 질의하고 결과를 가공하므로 별도 내부 상태 동기화가 거의 필요 없습니다. OSS는 **쿼리 결과 캐시**나 **인덱스 히트율** 등 자체 최적화 기법을 가질 수 있지만, 기본적으로 요청당 처리이므로 장애 발생 시 다른 인스턴스로 요청이 우회되면 큰 문제가 없습니다. 한편, OSS에서 **특정 쿼리가 지연**되거나 OSv2 응답이 느릴 경우, OSS는 해당 요청을 **타임아웃**시키거나 부분 실패 처리하고, 메트릭을 통해 SRE팀에 경고를 남깁니다. OSS와 OSv2 사이에는 서킷 브레이커가 적용되어, 만약 OSv2 측에 장애나 성능 문제가 발생하면 OSS는 일정 기간 요청을 차단하거나 축소하여 OSv2를 보호합니다. 또한 OSS 인스턴스는 무상태성이므로 **장애 격리**가 뛰어나, 일부 인스턴스 다운도 서비스 전체 가용성에 큰 영향을 주지 않으며, 자동 재기동/대체로 복구됩니다.

## **Actions Service (액션 서비스)**

**Actions 서비스**는 Foundry Ontology에서 **사용자 편집(User Edits)**을 **구조화된 액션**으로 적용해주는 마이크로서비스입니다. 사용자가 워크샵(Workshop)이나 Object Explorer 등 UI를 통해 객체 속성 값을 수정하거나 상태를 변경하는 작업을 하면, 이러한 **편집 요청을 하나의 “Action”**으로 캡처하여 Actions 서비스가 처리합니다. Actions 서비스는 **개별 객체 인스턴스 수준의 변경**을 수행하는데, 단순한 값 수정뿐만 아니라 **복잡한 조건 검증**이나 **권한 제어 로직**을 함께 적용할 수 있도록 설계되어 있습니다 . 예를 들어, 어떤 액션은 특정 조건(예: 관리 승인 필요)이 만족되어야만 실제로 객체 값을 바꾸도록 정의할 수 있고, 또 액션별로 세분화된 **Permissions**를 부여하여 어떤 사용자만 실행 가능하게 할 수도 있습니다 . Actions 서비스를 통해 이루어진 모든 변경은 **시계열 액션 로그**에 기록되어 사후에 누가 어떤 결정을 했는지 분석할 수 있고, 필요한 경우 **롤백(undo)**이나 **재현(replay)**도 가능합니다 ( Action Log와 Edit History 위젯 등을 통해 이를 관리)  .

Actions 서비스의 **API 경계**는 사용자의 의도를 받은 UI/클라이언트와 실제 데이터 변경을 수행하는 백엔드 사이에 **추상화 계층**을 형성하는 것입니다. 클라이언트는 단순히 “이 객체의 상태를 ‘고장’에서 ‘정상’으로 변경” 같은 고수준 요청을 /applyAction 형태로 보낼 수 있고, Actions 서비스는 내부에서 필요한 객체의 현재 버전 로드, 충돌 검증, 권한 체크 등을 수행한 뒤 **Funnel 서비스에 최종 변경사항을 전달**합니다 . 즉, Actions는 **트랜잭션 관리자** 역할을 하며 직접 DB를 고치지 않고 **Object Data Funnel**을 통해 **객체 저장소에 변경을 주입**합니다. OSv2에서는 **직접 DB 쓰기가 금지**되고 반드시 Actions 경유하도록 아키텍처가 바뀌었는데 , 이는 사용자 편집의 **일관성**과 **감사 추적**을 보장하기 위함입니다. Actions 서비스는 또한 **함수 연계**가 가능하여, 액션 실행 중에 미리 정의된 **Functions on Objects**를 호출해 복잡한 계산을 하거나 외부 API를 불러오는 등 **비즈니스 로직**을 확장할 수도 있습니다 .

**운영 및 복구 특성**으로, Actions 서비스 역시 **stateless**에 가깝게 동작하여 여러 인스턴스로 확장 가능합니다. 다만, 액션 적용 과정에서 **일시적으로 상태**(예: 객체의 버전 정보 등)를 확인하고 유지해야 하므로, 동일 액션 요청을 여러 서버가 처리하지 않도록 **동일 요청은 단일 인스턴스에서 처리**되도록 조정됩니다. Actions 서비스는 **동시 편집 충돌**을 막기 위한 **버전 관리 기법**을 도입하고 있습니다. 프런트엔드에서 편집을 시작한 시점의 객체 버전과, 실제 /apply시점의 객체 버전을 비교하여 불일치가 크면 **Stale Object** 에러로 중단하는 등, **낙관적 락(optimistic locking)** 개념으로 작동합니다  . OSv2의 경우 OSv1보다 이러한 일관성 검증을 **간소화**하여 성능을 높였지만 그만큼 일부 **약한 정합성**을 허용하는 방식입니다 . Actions 처리 중 오류나 실패 발생 시, 이미 완료된 부분은 **롤백**되며, 최종적으로 funnel에 변동 내역이 제출되지 않으면 실제 DB에 아무 영향도 주지 않고 종료됩니다. 또한, Actions 서비스가 일시 다운되면 사용자는 UI에서 액션 실행이 실패로 표시되고, **이미 큐에 전달된 편집**은 Funnel이 계속 처리하지만 **새로운 편집 시도는 실패**하게 됩니다. 이때에도 전체 Ontology 데이터는 안전하며, Actions만 재시작하면 정상 운영을 재개합니다. Actions → Funnel 호출에는 **재시도 로직**과 **내부 큐잉**으로 내구성이 확보되어, 일시적인 네트워크 문제 등에도 액션이 유실되지 않고 **최초 순서대로 처리**됩니다 .

## **Object Data Funnel (펀넬)**

**Object Data Funnel**, 줄여 **Funnel**은 Object Storage V2 아키텍처에서 **데이터 쓰기 경로를 오케스트레이션**하는 핵심 마이크로서비스입니다 . Funnel은 한마디로 **Ontology의 “인덱싱 파이프라인 관리자”**라고 할 수 있습니다. 구체적으로, 각종 **데이터소스**(데이터셋, 제한뷰, 스트리밍 데이터 등)로부터 데이터를 읽어와 **객체 인스턴스를 생성/갱신하는 작업**과, 앞서 설명한 **Actions 서비스로부터 전달된 사용자 편집 사항**을 수신하여 **객체 데이터에 반영하는 작업**을 책임집니다 . Funnel은 이러한 작업들을 **배치(batch)** 파이프라인과 **스트리밍(streaming)** 파이프라인의 두 가지 형태로 수행할 수 있습니다 . **배치 파이프라인**은 정기적으로 (예: 스케줄에 따라) 데이터소스를 스캔하여 변경분을 일괄 인덱싱하고, **스트리밍 파이프라인**은 실시간 데이터 소스(Kafka 등)로부터 이벤트를 지속 수신하여 작은 단위로 지속적으로 객체를 업데이트합니다 . 사용자는 데이터 특성과 요구 사항에 따라 배치 vs 스트리밍 인덱싱을 선택하여 Ontology를 최신 상태로 유지할 수 있습니다 .

Funnel 서비스는 여러 **하위 컴포넌트**로 구성되어 동작합니다. 우선 **변경 수집** 단계에서, 데이터소스의 변경이나 사용자 액션이 발생하면 Funnel은 이를 **이벤트 큐**에 기록합니다. Actions 서비스로부터 들어온 편집 명령은 **Funnel 내부 큐(queue)**에 적재되며, 여기에는 오프셋(offset) 기반으로 여러 동시 편집도 **순서 보장**을 하도록 설계되어 있습니다 . 이 큐는 **내구성**을 갖도록 관리되는데, Funnel은 자체적으로 몇 가지 Foundry **데이터셋(merge dataset 등)**을 운용하여 **원본 데이터 + 사용자 편집을 병합한 스냅샷**을 저장합니다 . 이를 통해 만약 Funnel이나 OSv2에 장애가 생겨도 **최근까지의 변경분을 모두 저장**해 두어 재처리할 수 있고, 또한 주기적으로 큐를 비워 **무한 성장**을 방지합니다 . Funnel은 데이터 이벤트를 순차적으로 처리하면서 **OMS로부터 메타데이터**를 참조하여 (예: 어떤 데이터 컬럼이 Ontology의 어떤 속성에 매핑되는지 등) **객체 인스턴스를 생성/갱신**하고, 최종적으로 **OSv2 객체 DB에 기록**합니다 . 이 과정에서 대량의 데이터가 있을 경우 Spark 등의 분산 처리로 병렬 인덱싱을 수행하거나, 데이터 특성에 따라 **증분 인덱싱** 최적화도 적용됩니다 . Funnel은 또한 **기존 인덱스와 최신 원본간 싱크**를 맞추는 역할도 하여, 일정 주기마다 데이터소스를 폴링하거나 변경 스트림을 들으면서 **언더라이잉 데이터소스의 업데이트를 Ontology에 전파**합니다 .

**API 분리 의의**로 볼 때, Funnel의 존재는 **데이터 쓰기 경로를 중앙 관제**한다는 데 있습니다. OSv2 Funnel 아키텍처에서는 데이터 소스로부터 객체DB로의 동기화가 **Funnel 서비스 하나를 통해** 이루어지므로, 다른 부분과 **느슨하게 결합**됩니다 . 예를 들어, 데이터 쓰기 부하가 높아도 Funnel이 이를 조절하여 순차 처리하거나 병렬 작업량을 조절하고, OSv2에 과도한 쓰기 부하가 가지 않도록 **큐 버퍼링**과 **백프레셔**를 관리할 수 있습니다. 또한 쓰기 경로의 문제 (예: 데이터소스 읽기 실패나 일시적 DB 다운 등)가 발생해도 Funnel이 **재시도**하거나 **대기**하면서 전체 일관성을 유지하고, 오류를 격리시켜 다른 서비스에 영향이 번지지 않게 합니다. OMS와 Funnel의 경계도 분명하여, **스키마 변화**가 생기면 Funnel은 해당 변경 이벤트를 감지해 새로운 속성에 대한 인덱싱을 수행하거나, 호환 안 되는 데이터는 제외하는 등 **런타임에 파이프라인을 재구성**합니다.

**운영 관점**에서 Funnel은 **stateless**에 가깝지만, 내부적으로 관리하는 **큐와 임시 데이터셋** 때문에 약간의 **상태ful** 성격도 있습니다. Funnel 서비스 자체는 여러 인스턴스를 둘 수 있으며, 일반적으로 **객체 타입별 파이프라인** 단위로 병렬 처리를 수행하거나, **데이터 소스별 병렬 작업**을 처리하도록 만들어집니다. 대규모 설치 환경에서는 데이터소스/객체타입 별로 작업을 분산 처리하기 위해 여러 Funnel worker들이 역할을 나누거나, 메시지 큐 시스템(Kafka 등)을 활용해 **컨슈머 스케일아웃**을 할 수도 있습니다. Funnel이 처리 중 장애를 일으키면, 해당 인덱싱 작업은 실패로 표시되고 이후 재시작 시 **마지막 커밋 오프셋**부터 재처리하도록 설계되었습니다. 이때도 원본 데이터는 안전하고, 중복 처리를 피하기 위한 **멱등성** 처리도 포함됩니다. Funnel과 OSv2 사이, 또는 Funnel과 데이터 소스 사이에는 백업 경로(예: 임시 파일 저장 등)가 있어, 일시적으로 대상이 불가용하면 변경 이벤트를 **임시 저장**해 두고 후에 재적용합니다. 이러한 **큐잉과 버퍼링** 메커니즘, 그리고 서킷 브레이커를 통한 **쓰로틀링**, **재시도** 전략으로 Funnel은 데이터 파이프라인의 **내고장성(fault-tolerance)**을 높이고 Ontology 데이터를 항상 최신에 가깝게 유지해줍니다 .

---

### **“쓰기(write)“의 의미**

### **1. 객체 인스턴스 생성**

- 예: Customer, Order, Shipment 같은 **Object Type**에 해당하는 **실제 객체를 생성**
- 데이터셋에서 레코드 한 줄(row)을 읽어와 Customer(id=123, name="Alice") 같은 **객체 인스턴스**로 변환
- 이것이 **OSv2 객체 DB에 저장**됨

### **2. 기존 객체의 갱신(update)**

- 예: 기존 고객 정보의 주소나 이메일이 바뀌면 → 해당 필드만 업데이트
- 또는, Order 객체에 새로운 배송 상태 필드가 추가됨 → 이에 맞춰 객체 구조를 리빌드

### **3. 사용자 액션의 반영**

- 예: 사용자가  UI에서 객체 필드를 직접 수정하거나, 수동 입력 시
- 이 또한 Funnel을 통해 객체 DB에 반영되는 “쓰기”로 처리됨

---

### **기술적으로 말하는 쓰기(write)와는 다름**

 쓰기는 단순한 INSERT나 UPDATE SQL이 아니다.

대신 다음 단계를 포함하는 **고차원 쓰기 파이프라인**임:

| **단계** | **설명** |
| --- | --- |
| 1️⃣ 추출 (Extract) | 데이터셋, 제한뷰, 스트림 등 다양한 소스로부터 데이터 수신 |
| 2️⃣ 매핑 (Map) | **Ontology의 스키마 메타데이터**를 참고하여 어떤 데이터가 어떤 객체 속성에 매핑되는지 결정 |
| 3️⃣ 병합 (Merge) | 기존 객체가 있다면 병합 로직 적용 (중복 체크, 버전 비교 등) |
| 4️⃣ 커밋 (Commit) | 결과를 **Object Storage V2**에 저장 (→ 이게 진짜 물리적 “쓰기”) |
| 5️⃣ 이벤트 발행 | 성공 시 object.updated, object.created 등의 도메인 이벤트를 후속 서비스에 발행 |

---

### **비유로 이해하면:**

> “쓰기”는 단순히 엑셀에 셀 하나 바꾸는 게 아니라,
>

> ERP 시스템에서 고객정보 마스터 데이터를 갱신하고,
>

> 이 정보가 모든 다른 시스템에도 자동 반영되게 만드는 전체 흐름을 말함.
>

---

### **💡 핵심 요약**

| **관점** | **“쓰기” 의미** |
| --- | --- |
| **기능** | 객체 인스턴스 생성/수정 |
| **소스** | 데이터셋, 제한뷰, 스트리밍, 사용자 액션 등 |
| **결과** | OSv2 객체 저장소에 객체 데이터가 업데이트됨 |
| **담당자** | 이 전체 쓰기 과정을 관리하는 것이 바로 **Funnel** |

---

## **Functions on Objects (객체 함수)**

**Functions on Objects** 서비스(줄여 **Functions**)는 Ontology에 정의된 **객체와 연결된 코드 실행**을 지원하는 마이크로서비스입니다. 이는 사용자가 직접 작성한 비즈니스 로직(예: 특정 객체의 여러 속성을 계산하여 결과 도출, 또는 객체 간 커스텀 검증 로직 등)을 Ontology와 밀접하게 연계하여 실행할 수 있게 해주는 기능입니다 . Foundry에서는 **대시보드, 워크숍 등의 운영 맥락**에서 빠르게 실행되어야 하는 간단한 로직을 Functions로 작성할 수 있도록 하고 있는데, 주로 **TypeScript 또는 Python**으로 함수를 작성하여 Ontology에 배포하면 해당 함수를 **API처럼 호출**하거나 **액션 내에서 활용**할 수 있습니다  . 예를 들어, 대시보드에서 사용자 클릭 시 특정 객체 집합에 대해 커스텀 산식을 적용해 결과를 보여주거나, 액션 수행 시 복잡한 산출을 위해 함수로직을 호출하는 식입니다. Functions 서비스는 이러한 함수를 **미리 컴파일/저장**하고 있다가 호출 요청이 들어오면 **신속히 실행하고 결과를 반환**합니다.

Functions on Objects의 **API 경계**는 일종의 **함수 실행 플랫폼**으로,  다른 부분 (예: UI나 액션)이 객체 ID나 Object Set 등을 인자로 함수를 호출하면, Functions 서비스가 해당 코드를 실행하는 구조입니다. 이때 함수는 내부적으로 **OSS를 통해 객체 데이터를 조회**하거나, **다른 서비스 API**를 호출할 수도 있지만, 이러한 디테일은 함수 코드에 캡슐화되어 있습니다. 즉, Functions 서비스는 **코드 실행 환경을 표준화**하여, 개발자가 비즈니스 로직 구현에 집중할 수 있게 하고, 실행 시에는 **객체 컨텍스트**를 자동으로 주입하거나 결과를 Ontology와 연계된 형태로 돌려줍니다. Functions는 Foundry 플랫폼의 **확장 포인트**로 볼 수 있으며, Ontology에 없는 특수 연산이나 서드파티 연계 등을 구현하는 데 활용됩니다.

**운영 측면**에서 Functions 서비스는 **서버리스 함수 실행 엔진**과 유사하게 동작합니다. 함수별로 격리된 **샌드박스 환경(예: 가상머신 또는 컨테이너)**에서 코드를 실행하여 한 함수의 에러나 오작동이 다른 부분에 영향주지 않게 합니다. 함수 호출량이 증가하면 Functions 서비스는 내부적으로 더 많은 실행 워커를 띄워 **자동 확장**하거나, 호출을 **큐잉**하여 부하를 조절합니다. Functions 자체는 상태를 유지하지 않으므로 여러 인스턴스를 병렬로 운영해도 무방하며, 이로써 대규모 동시 호출에도 대응할 수 있습니다. 함수 실행 도중 오류가 발생하면 해당 요청에 한해 스택트레이스를 반환하고 격리 종료하며, 서비스 전체에는 영향을 주지 않습니다. 함수 코드에 대한 **버전관리와 모니터링**을 제공하는데, 함수의 **성능 메트릭(응답시간, 호출빈도)**이나 **오류 로그**를 수집하여 문제 시 개발자가 개선할 수 있게 합니다. Functions 서비스는 다른 마이크로서비스와 달리 **사용자 커스텀 코드**를 실행한다는 점에서, 리소스 사용이나 무한 루프 등에 대한 **제한(타임아웃, 메모리/CPU 쿼터)**을 두어 플랫폼 안정성을 지킵니다. 전반적으로 Functions on Objects는 **온톨로지와 사용자 코드 사이의 경계 계층**으로서, 안전하고 빠르게 비즈니스 로직을 주입할 수 있게 해주는 서비스입니다.

## **서비스 간 관계 및 데이터 흐름**

Ontology를 구성하는 각 서비스는 위와 같이 저마다의 역할을 가지고 있지만, **Ontology 데이터의 라이프사이클** 상 긴밀하게 상호작용합니다. 그림으로 요약하면 아래와 같습니다:

*Palantir Foundry Object Storage V2 아키텍처: 데이터 소스에서 Ontology로의 쓰기 경로와 읽기 경로가 분리되어 있다. OMS는 전체 Ontology의 스키마 메타데이터를 제공하고, 원천 데이터는 Funnel을 통해 OSv2 객체 데이터베이스에 인덱싱된다. Object Set Service (OSS)는 객체 데이터베이스로부터 객체들을 검색/집계하여 애플리케이션에 제공하며, Actions 서비스는 사용자 편집을 받아 Funnel에 전달함으로써 객체 데이터를 갱신한다. Functions on Objects 서비스는 필요 시 OSS를 통해 객체 데이터를 읽어 사용자 정의 로직을 실행하거나, 액션에서 함수를 호출하여 복잡한 처리를 수행할 수 있다. 이처럼 각 서비스가 맡은 책임을 통해 Ontology 데이터의 **읽기/쓰기 경로가 분리**되고 **기능별로 모듈화**되어 있다.*

위 다이어그램과 같이, **서비스 간 호출 흐름**은 다음과 같이 정리됩니다:

| **출발 (요청 발생)** | **대상 서비스** | **상호 작용 및 데이터 흐름** |
| --- | --- | --- |
| **데이터소스**
(데이터셋, 스트림 등) | **Object Data Funnel** | 데이터 파이프라인에서 나온 **신규/갱신 레코드**는 이벤트 형태로 Funnel에 전달됩니다. Funnel은 이를 받아 해당 **객체 타입**의 인스턴스로 변환하고 OSv2에 **인덱싱**합니다 . 배치 파이프라인의 경우 스케줄러가 데이터 변화를 감지해 Funnel을 트리거하고, 스트리밍의 경우 지속적으로 Funnel에 이벤트가 흘러갑니다. |
| **Ontology 메타데이터 변경**
(OMS) | **Object Data Funnel** | 사용자가 Ontology Manager 등을 통해 **스키마를 수정**하면 OMS에 반영되고, OMS는 이러한 변경(예: 새로운 속성 추가)을 **이벤트로 브로드캐스트**합니다. Funnel은 이 이벤트를 수신하여 **인덱싱 스키마를 재설정**하거나, 필요한 재색인 작업을 수행합니다. 예를 들어, 새 속성이 추가되면 Funnel이 해당 속성을 포함하도록 객체 데이터셋을 다시 빌드합니다. |
| **사용자 액션 요청**
(예: 객체 편집) | **Actions 서비스** | 사용자가 UI에서 객체를 수정하면 해당 요청이 Actions 서비스의 **API (/apply)**로 들어옵니다. Actions 서비스는 요청에 정의된 논리를 검증/실행하여, 최종 **객체 변경 지시**를 생성합니다 . |
| **Actions 편집 지시** | **Object Data Funnel** | 액션으로 확정된 객체 수정 사항은 Actions 서비스가 Funnel의 API로 **편집 명령(instruction)**을 보냅니다. Funnel은 이를 자체 **큐에 저장**하고, 순서를 보장하면서 OSv2에 적용합니다 . 이 과정에서 Funnel은 OSv2의 해당 객체 **인덱스를 즉시 업데이트**하므로, 편집 직후의 조회에서도 반영된 값을 볼 수 있습니다 . 또한 편집 내용은 **병합 데이터셋**에 주기적으로 flush되어 영구 보존됩니다 . |
| **Funnel 인덱싱/편집** | **Object Storage V2** | Funnel 서비스는 수집한 **원본 데이터 변경**과 **사용자 편집**을 OSv2 **객체 데이터베이스**들에 **반영**합니다 . 여기에는 새로운 객체의 생성, 기존 객체 속성 값 변경, 링크 관계 추가/제거 등이 모두 포함됩니다. Funnel은 OSv2의 적절한 저장 엔진(API)를 호출하여 데이터를 쓰며, OSv2는 변경 내용을 **인메모리 인덱스 및 디스크** 등에 기록합니다. 만약 OSv2 쪽에 일시적 장애가 있으면 Funnel의 큐에 이벤트를 대기시켜 **재시도**하며, 성공적으로 쓰여질 때까지 보장합니다. |
| **애플리케이션의 객체 질의** (검색, 필터 등) | **Object Set Service (OSS)** | 대시보드나 외부 API 클라이언트가 특정 객체나 객체 목록을 조회하려 할 때, OSS의 **질의 API**를 호출합니다 . 쿼리에는 객체 타입, 필터 조건, 필요 속성, 집계 함수 등이 포함될 수 있습니다. |
| **OSS 객체 조회 요청** | **Object Storage V2** | OSS는 들어온 질의를 해석하여 OSv2의 **전용 쿼리 엔진**에 전달합니다. 예를 들어 “Orders 객체 중 상태가 ‘지연’인 것의 수량 합계” 질의는 OSS가 OSv2에 해당 조건의 객체들을 검색하고 합계를 계산하도록 요청합니다. OSv2는 **사전 구축된 인덱스**나 **분산 쿼리**를 활용해 결과를 산출하고, OSS에 반환합니다. OSv2로부터 데이터를 받을 때 OSS는 **권한** 검증을 추가로 적용하여, 사용자가 볼 수 없는 필드는 제거하거나 결과를 필터링합니다. |
| **OSS → Functions** (데이터 제공) | **Functions on Objects** | 사용자가 대시보드 등에서 **객체 함수**를 호출하면, Functions 서비스는 해당 함수의 코드 실행을 준비하며 필요시 **OSS를 통해 데이터**를 가져옵니다. 예를 들어 함수가 특정 Object Set을 인자로 받아 동작한다면, Functions 서비스는 그 Object Set의 객체 데이터를 OSS로부터 조회하여 함수에 전달합니다. (Functions → OSS 호출은 내부적으로 일어나며, Functions 코드에서 SDK 등을 통해 객체를 로드하면 실제로는 OSS API가 호출됩니다.) |
| **Functions → Actions** (함수 내 액션) | **Actions 서비스** | (선택적) 함수 로직에서 **다른 액션을 트리거**할 수도 있습니다. **Function-backed Action** 개념처럼, 함수가 수행한 연산 결과에 따라 새로운 편집 액션을 호출하면 Functions 서비스가 Actions API를 부르는 식입니다 . 이 경우 함수 → Actions → Funnel의 흐름이 연쇄적으로 일어나 추가적인 객체 변경을 수행합니다. (예: 계산 결과 임계치 초과 시 경고 상태로 객체 업데이트 액션 실행 등). |

위 상호 작용에서 볼 수 있듯이, OMS는 **메타데이터 이벤트** 제공자, Funnel은 **쓰기 처리자**, OSS는 **읽기 제공자**, Actions는 **편집 트랜잭션 관리자**, Functions는 **사용자 로직 실행자**로 서로 협력합니다. 이러한 구조는 **쓰기 경로**(Datasource/Actions → Funnel → OSv2)와 **읽기 경로**(OSv2 → OSS → 애플리케이션)를 분리함으로써, 쓰기 부하와 읽기 부하를 **독립적으로 처리**할 수 있게 하고, 특정 기능에 문제가 생겨도 경계 밖으로 영향이 확산되지 않도록 **격리**합니다  . 또한 각 서비스는 OMS를 공통 참조함으로써 **단일 메타데이터 소스**를 사용하고, OSS를 통해서만 OSv2에 접근함으로써 **데이터 접근 제어**와 **로그 추적**이 용이합니다.

## **서비스 배포 및 운영 특성**

이들 구성 요소는 모두 **마이크로서비스**로 구현되어 플랫폼 내에서 개별적으로 배포되고 관리됩니다  . Palantir의 **Skylab/Apollo** 배포 인프라는 수백 개에 이르는 마이크로서비스를 체계적으로 관리하며, Ontology 관련 서비스들도 각기 **SLS 패키지**(서비스 레이아웃 사양)로 묶여 필요 인스턴스 수만큼 프로비저닝됩니다  . 각 서비스의 운영상 특징을 요약하면 다음과 같습니다:

- **OMS**: *Deployment & Scaling:* 경량 서비스로 **1~N개 인스턴스**로 배포 가능하며, 메타데이터 DB 를 사용해 **상태를 저장**합니다. 큰 부하가 없는 편이라 일반적으로 소수 인스턴스로 충분하지만, 고가용성을 위해 이중화될 수 있습니다. *Fault Isolation:* OMS 장애 시 새로운 스키마 변경만 일시 지연되고, **캐시에 있는 메타데이터로 다른 서비스는 임시 동작**하므로 영향이 제한적입니다. 재기동 시 이전 DB 상태로 빠르게 복구됩니다. |
- **OSv2**: *Deployment & Scaling:* **분산 데이터베이스 클러스터** 형태로 여러 노드에 걸쳐 배포됩니다. 데이터 양과 조회 부하에 맞춰 노드 증설로 **수평 확장**하며, 인덱스 노드와 쿼리 노드 등을 역할별로 나눌 수 있습니다 . *Fault Isolation:* 노드 장애 시 해당 shard의 데이터는 **다른 노드에 복제**되었거나, 최악의 경우 Funnel이 **재색인**하여 복원합니다. 부분 장애는 OSS가 감지하여 해당 노드를 쿼리 대상에서 제외(**서킷 브레이커**)하고, Funnel도 장애 노드에 기록 시 재시도/대기함으로써 전체 정합성을 유지합니다. |
- **OSS**: *Deployment & Scaling:* **Stateless** 웹 서비스로 여러 인스턴스를 활성화해 **로드밸런싱** 처리합니다. 각 인스턴스는 독립적으로 OSv2에 질의 가능하여, 사용자 수 증가에 쉽게 대응할 수 있습니다. *Fault Isolation:* 특정 OSS 인스턴스 장애는 로드밸런서가 트래픽을 다른 인스턴스로 보내면 되고, 세션 상태가 없으므로 영향이 미미합니다. OSv2 또는 권한 서비스 등 하위 시스템 문제에 따른 예외는 OSS에서 **적절한 오류 처리**로 격리됩니다. |
- **Actions**: *Deployment & Scaling:* 액션 서비스도 **여러 인스턴스**로 확장 가능하며, 동시에 많은 액션 요청(대규모 일괄 편집 등)을 처리할 수 있도록 설계되었습니다 . 단일 액션 요청은 한 인스턴스가 끝까지 처리하여, DB충돌 검증 일관성을 지킵니다. *Fault Isolation:* 액션 처리 중 에러가 나면 해당 요청만 실패하고, 다른 액션에는 영향 없습니다. Funnel에 이미 보낸 편집은 큐잉되므로, 액션 직후 서비스 장애가 생겨도 **이미 큐에 있는 변경은 유실되지 않습니다** .
- **Funnel**: *Deployment & Scaling:* Funnel은 **다중 스레드/프로세스**로 동작하거나, 객체 타입별 여러 워커 인스턴스를 둘 수 있습니다. 데이터 파이프라인의 확장에 맞춰 **병렬 처리 파이프라인** 수를 늘리거나, 경우에 따라 **여러 Funnel 서비스 그룹**을 운용해 부하를 분산시킵니다. *Fault Isolation:* Funnel은 내부 큐와 체크포인트를 통해 **한번 받은 데이터는 확실히 처리**하는 것을 보장합니다. 일부 작업자 실패 시 해당 작업만 재시도하면 되고, 전체 장애 시에도 병합 데이터셋에 기록된 마지막 상태로부터 **재실행**이 가능합니다. 또한 Funnel이 과부하될 경우 스스로 큐 적체를 인지하고 **백프레셔**를 걸거나, 데이터 생산 측(스케줄러)에 **신호를 보내 속도를 조절**합니다. 이는 쓰기 경로의 폭주로 인한 시스템 장애를 미연에 방지합니다. |
- **Functions**: *Deployment & Scaling:* Functions 서비스는 **서버리스 플랫폼**에 가깝게, 호출량에 따라 **컨테이너 혹은 스레드 풀**을 동적으로 조절합니다. 다수 사용자가 동시에 함수를 실행하면 여러 함수를 병렬로 수행하도록 인프라가 확장되며, 필요 시 특정 함수별로 리소스 제한을 높이거나 별도 워커를 둘 수도 있습니다. *Fault Isolation:* 함수 실행 중 오류/예외는 해당 함수 호출에만 영향을 주고 격리됩니다. 무한루프나 과다자원 사용은 타임아웃과 제한으로 자동 중단되어, 서비스 전체에 지장을 주지 않습니다. Functions 서비스 자체 장애 시에는 일시적으로 함수 호출이 실패하지만, 시스템의 핵심 데이터 흐름에는 영향을 주지 않으며 (함수는 부가 기능), 재기동 후 호출을 재개할 수 있습니다.

요약하면, 각 서비스는 **마이크로서비스로서 독립 배포**되어 필요에 따라 **수평 확장** 가능하고, **서로 격리된 실패 영역**을 가집니다.관리자는 서비스별 **헬스 체크**와 **자동 롤아웃/롤백**을 통해 지속적으로 서비스 상태를 관찰하며, 성능 저하나 오류 증가 시 신규 버전 배포를 **중단하거나 롤백**하여 안정성을 확보합니다 . 또한  마이크로서비스 특성상 발생할 수 있는 분산 시스템 이슈 (네트워크 지연, 부분 실패 등)에 대비해 각 서비스 간 통신에 **표준 RPC 프레임워크**와 **에러 처리 패턴**(재시도, 폴백 등)을 적용해 두었습니다.

## **모니터링 및 관측(Observability) 체계**

플랫폼은 수백개의 마이크로서비스로 이루어진 만큼, **모니터링과 관측** 체계를 매우 중시합니다 . Ontology 관련 서비스들도 공히 **로그(logging)**, **메트릭(metrics)**, **분산 트레이싱(tracing)** 측면에서 촘촘히 계측되어 운영됩니다. 각 서비스는 **표준화된 구조화 로그**를 생성하는데, Palantir는 모든 서비스의 로그를 JSON 형태로 작성하고 (호스트, 타임스탬프, 로그 레벨, 메시지 등 공통 필드 포함) 중앙 수집기로 보내는 체계를 갖추고 있습니다  . 구체적으로, 각 서비스 호스트에는 **로그 수집 에이전트**가 있어 애플리케이션 로그를 파일/표준출력에서 읽어 **Kafka/Kinesis**와 같은 **글로벌 스트림**으로 전송합니다 . 중앙의 **텔레메트리 인프라**는 이 로그 스트림을 받아 환경별로 필터링하고 스키마를 표준화한 뒤, **인덱싱**하여 저장합니다  .과거 Elasticsearch 기반으로 로그 검색 인프라를 구축했으나, 서비스와 배포 수 증가로 자체 Lucene 기반 인덱싱 솔루션으로 발전시켰습니다  . 이를 통해 모든 마이크로서비스의 로그를 수십 테라바이트 규모까지 안정적으로 저장하고, **실시간 검색**과 **대시보드 시각화**(예: Kibana 또는 자체 툴)를 구현했습니다. SRE나 개발자는 특정 시간대의 Funnel 오류 로그나, Actions API 호출 시퀀스를 **추적 조회**하여 문제 원인을 진단할 수 있습니다.

**메트릭 수집**도 각 서비스별로 이루어집니다. 내부적으로 Prometheus와 Grafana같은 툴을 사용하거나, Datadog과 연계하여 **시계열 메트릭** (CPU, 메모리 사용량, API 응답시간, 큐 길이, 오류율 등)을 모니터링합니다 . Ontology 서비스들은 예를 들어 **Funnel의 인덱싱 지연 시간**, **OSS의 쿼리 응답시간 분포**, **Actions의 초당 처리 건수**, **OSv2의 메모리 인덱스 히트율** 등의 지표를 지속적으로 emit합니다. 이러한 메트릭들은 중앙 대시보드에서 시각화되어 운영자가 **서비스 상태와 성능 추세**를 한눈에 볼 수 있게 합니다시스템은 특히 **업데이트 배포 시** 성능 메트릭과 오류율을 자동 관찰하여, 새 버전이 기준치 이상 문제를 일으키면 배포를 중단하거나 롤백하는 **자동 판결(adjudication)**을 수행합니다 . 이는 각 서비스의 메트릭이 얼마나 신뢰성있게 수집되는지, 그리고 운영에 적극 활용되고 있는지를 보여주는 사례입니다.

- *분산 트레이싱(Distributed Tracing)**도  활용됩니다.  **OpenTracing/Zipkin**과 같은 표준을 도입하여 마이크로서비스 간 호출 흐름을 추적하고 있다고 언급되어 있습니다 . 실제로 Ontology의 한 요청 예를 들면, 사용자가 대시보드에서 객체를 편집할 때 Actions → Funnel → OSv2까지 이어지는 호출 체인을 각각의 Trace ID로 묶어 추적할 수 있습니다. 각 서비스는 요청 헤더나 메시지에 **Trace 컨텍스트**를 전달하고 로그에도 해당 ID를 남김으로써, 추후 Zipkin 등으로 **end-to-end 트랜잭션 추적**이 가능합니다. 이를 통해 특정 요청이 어디서 지연됐는지 (예: Funnel 단계에서 대기), 어디서 에러났는지 (예: OSv2 shard 3에서 오류) 파악할 수 있어 장애 분석에 큰 도움이 됩니다 .  이러한 **분산 관측성** 도구들을 종합해 **텔레메트리 대시보드**를 구축해왔으며, 마이크로서비스의 리소스 사용 패턴, 성능 병목을 찾아내 지속적으로 아키텍처를 개선하고 있습니다 .

또한 **도메인 특화 모니터링**도 제공합니다. 예를 들어 **Ontology Monitoring Views**라는 기능으로 특정 Ontology 객체 타입의 **Sync 상태**나 **Edit 처리량** 등을 모니터링할 수 있고 , **Action Metrics** 기능으로 액션별 평균 실행시간, 성공/실패 건수를 추적하기도 합니다  . 이처럼 플랫폼 전반에 걸친 기술적 모니터링과, Ontology 도메인에 특화된 지표 모니터링을 결합하여, 운영자는 **시스템 수준 건강상태**와 **비즈니스 로직 수준 상태**를 모두 파악할 수 있습니다.

**로그, 메트릭, 트레이스**의 삼각측량으로 이루어진  observability 체계는, Ontology 마이크로서비스들이 대규모 분산 환경에서도 신뢰성 있게 동작할 수 있는 기반입니다. 각 서비스는 문제 발생 시 경고 알람(pager duty)을 발생시키도록 임계치가 설정되어 있고 , 인프라팀은 24/7 이 알람에 대응하여 심각한 장애를 미연에 방지하거나 신속 복구합니다. 결과적으로, **모듈화된 아키텍처**와 함께 **견고한 운영/모니터링 체계**를 갖추어, 엔터프라이즈 환경에서 **확장성, 성능, 신뢰성**을 모두 만족하는 지식 통합 레이어를 실현하고 있습니다.
