#!/usr/bin/env python3
"""
ì‹œë§¨í‹± ì¤‘ë³µ ì½”ë“œ ë¶„ì„ê¸° - Ultra Deep Analysis
ê¸°ëŠ¥ì ìœ¼ë¡œ, ë§¥ë½ì ìœ¼ë¡œ ê°™ì€ ì—­í• ì„ í•˜ëŠ” ì½”ë“œë“¤ì„ ì°¾ì•„ë‚´ëŠ” ì‹¬ì¸µ ë¶„ì„ ë„êµ¬

ë‹¨ìˆœí•œ ì½”ë“œ ì¤‘ë³µì´ ì•„ë‹Œ, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ê³¼ ì•„í‚¤í…ì²˜ ë ˆë²¨ì—ì„œì˜ ì˜ë¯¸ì  ì¤‘ë³µì„ íƒì§€
"""

import os
import ast
import json
import re
from pathlib import Path
from typing import Dict, List, Set, Any, Tuple
from collections import defaultdict
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SemanticDuplicateAnalyzer:
    """ì˜ë¯¸ì  ì¤‘ë³µ ë¶„ì„ê¸°"""
    
    def __init__(self, root_dir: str):
        self.root_dir = Path(root_dir)
        
        # ê¸°ëŠ¥ë³„ íŒ¨í„´ ì €ì¥ì†Œ
        self.auth_patterns = defaultdict(list)          # ì¸ì¦ ê´€ë ¨ ê¸°ëŠ¥ë“¤
        self.validation_patterns = defaultdict(list)    # ê²€ì¦ ë¡œì§ë“¤
        self.database_patterns = defaultdict(list)      # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°/ì¿¼ë¦¬ë“¤
        self.api_patterns = defaultdict(list)           # API ì—”ë“œí¬ì¸íŠ¸ íŒ¨í„´ë“¤
        self.error_handling_patterns = defaultdict(list) # ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´ë“¤
        self.config_patterns = defaultdict(list)        # ì„¤ì • ê´€ë¦¬ íŒ¨í„´ë“¤
        self.logging_patterns = defaultdict(list)       # ë¡œê¹… íŒ¨í„´ë“¤
        self.serialization_patterns = defaultdict(list) # ì§ë ¬í™”/ì—­ì§ë ¬í™” íŒ¨í„´ë“¤
        self.business_logic_patterns = defaultdict(list) # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ íŒ¨í„´ë“¤
        self.integration_patterns = defaultdict(list)   # ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ë™ íŒ¨í„´ë“¤
        
        # ì•„í‚¤í…ì²˜ ë ˆë²¨ íŒ¨í„´ë“¤
        self.service_patterns = defaultdict(list)       # ì„œë¹„ìŠ¤ ë ˆì´ì–´ íŒ¨í„´ë“¤
        self.repository_patterns = defaultdict(list)    # ë ˆí¬ì§€í† ë¦¬ íŒ¨í„´ë“¤
        self.middleware_patterns = defaultdict(list)    # ë¯¸ë“¤ì›¨ì–´ íŒ¨í„´ë“¤
        self.model_patterns = defaultdict(list)         # ëª¨ë¸/ìŠ¤í‚¤ë§ˆ íŒ¨í„´ë“¤
        
        # ë°ì´í„° í”Œë¡œìš° íŒ¨í„´ë“¤
        self.transformation_patterns = defaultdict(list) # ë°ì´í„° ë³€í™˜ íŒ¨í„´ë“¤
        self.filtering_patterns = defaultdict(list)     # ë°ì´í„° í•„í„°ë§ íŒ¨í„´ë“¤
        self.aggregation_patterns = defaultdict(list)   # ë°ì´í„° ì§‘ê³„ íŒ¨í„´ë“¤
        
    def analyze_codebase(self) -> Dict[str, Any]:
        """ì½”ë“œë² ì´ìŠ¤ ì „ì²´ ì˜ë¯¸ì  ë¶„ì„"""
        logger.info("ğŸ§  ì˜ë¯¸ì  ì¤‘ë³µ ë¶„ì„ ì‹œì‘ - ì‹¬ì¸µ ë¶„ì„ ëª¨ë“œ")
        
        # ì£¼ìš” ì„œë¹„ìŠ¤ ë””ë ‰í† ë¦¬ë“¤
        service_dirs = [
            "user-service",
            "audit-service", 
            "ontology-management-service",
            "arrakis-common"
        ]
        
        all_files = []
        for service_dir in service_dirs:
            service_path = self.root_dir / service_dir
            if service_path.exists():
                for py_file in service_path.rglob("*.py"):
                    if self._is_relevant_file(py_file):
                        all_files.append(py_file)
        
        # ë£¨íŠ¸ì˜ ì¤‘ìš”í•œ íŒŒì¼ë“¤ë„ í¬í•¨
        for py_file in self.root_dir.glob("*.py"):
            if self._is_relevant_file(py_file):
                all_files.append(py_file)
        
        logger.info(f"ğŸ“ {len(all_files)}ê°œ í•µì‹¬ íŒŒì¼ ë¶„ì„")
        
        # ê° íŒŒì¼ì˜ ì˜ë¯¸ì  íŒ¨í„´ ë¶„ì„
        for file_path in all_files:
            self._analyze_file_semantics(file_path)
        
        # ì˜ë¯¸ì  ì¤‘ë³µ íƒì§€
        semantic_duplicates = self._detect_semantic_duplicates()
        
        return semantic_duplicates
    
    def _is_relevant_file(self, file_path: Path) -> bool:
        """ë¶„ì„ ëŒ€ìƒ íŒŒì¼ì¸ì§€ í™•ì¸"""
        # ì œì™¸í•  íŒ¨í„´ë“¤
        exclude_patterns = [
            '__pycache__',
            '.git',
            'venv',
            'env',
            '.pytest_cache',
            'migrations',
            '__init__.py',
            'test_',  # í…ŒìŠ¤íŠ¸ íŒŒì¼ì€ ì¼ë‹¨ ì œì™¸
            '.pyc'
        ]
        
        file_str = str(file_path)
        return not any(pattern in file_str for pattern in exclude_patterns)
    
    def _analyze_file_semantics(self, file_path: Path):
        """íŒŒì¼ì˜ ì˜ë¯¸ì  íŒ¨í„´ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            file_context = self._analyze_file_context(file_path, content)
            
            # AST ìˆœíšŒí•˜ë©° ì˜ë¯¸ì  íŒ¨í„´ ì¶”ì¶œ
            for node in ast.walk(tree):
                self._extract_semantic_patterns(node, file_path, content, file_context)
                
        except Exception as e:
            logger.debug(f"íŒŒì¼ ë¶„ì„ ê±´ë„ˆëœ€ {file_path}: {e}")
    
    def _analyze_file_context(self, file_path: Path, content: str) -> Dict[str, Any]:
        """íŒŒì¼ì˜ ì „ì²´ì ì¸ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        context = {
            'file_path': str(file_path),
            'is_service': 'service' in str(file_path).lower(),
            'is_repository': 'repository' in str(file_path).lower() or 'repo' in str(file_path).lower(),
            'is_model': 'model' in str(file_path).lower() or 'schema' in str(file_path).lower(),
            'is_api': 'api' in str(file_path).lower() or 'route' in str(file_path).lower(),
            'is_middleware': 'middleware' in str(file_path).lower(),
            'is_config': 'config' in str(file_path).lower() or 'setting' in str(file_path).lower(),
            'is_auth': 'auth' in str(file_path).lower() or 'jwt' in str(file_path).lower(),
            'is_audit': 'audit' in str(file_path).lower(),
            'service_type': self._determine_service_type(file_path),
            'imports': self._extract_imports(content),
            'external_dependencies': self._extract_external_dependencies(content)
        }
        
        return context
    
    def _determine_service_type(self, file_path: Path) -> str:
        """ì„œë¹„ìŠ¤ íƒ€ì… ê²°ì •"""
        path_str = str(file_path).lower()
        
        if 'user-service' in path_str or 'user_service' in path_str:
            return 'user_service'
        elif 'audit-service' in path_str or 'audit_service' in path_str:
            return 'audit_service'
        elif 'ontology' in path_str or 'oms' in path_str:
            return 'ontology_service'
        elif 'arrakis-common' in path_str:
            return 'common_library'
        else:
            return 'unknown'
    
    def _extract_imports(self, content: str) -> List[str]:
        """ì¤‘ìš”í•œ importë“¤ ì¶”ì¶œ"""
        important_imports = []
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('import ') or line.startswith('from '):
                # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ì‹ë³„
                if any(lib in line for lib in ['fastapi', 'sqlalchemy', 'jwt', 'bcrypt', 'redis', 'httpx', 'pydantic']):
                    important_imports.append(line)
        
        return important_imports
    
    def _extract_external_dependencies(self, content: str) -> List[str]:
        """ì™¸ë¶€ ì˜ì¡´ì„± íŒ¨í„´ ì¶”ì¶œ"""
        dependencies = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° íŒ¨í„´
        if re.search(r'database|db|Database|DB', content):
            dependencies.append('database')
        
        # Redis ì‚¬ìš© íŒ¨í„´
        if re.search(r'redis|Redis', content):
            dependencies.append('redis')
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ íŒ¨í„´
        if re.search(r'httpx|requests|aiohttp', content):
            dependencies.append('http_client')
        
        # JWT ê´€ë ¨ íŒ¨í„´
        if re.search(r'jwt|JWT|token|Token', content):
            dependencies.append('jwt')
        
        return dependencies
    
    def _extract_semantic_patterns(self, node: ast.AST, file_path: Path, content: str, file_context: Dict[str, Any]):
        """ì˜ë¯¸ì  íŒ¨í„´ ì¶”ì¶œ"""
        
        if isinstance(node, ast.FunctionDef):
            self._analyze_function_semantics(node, file_path, content, file_context)
        elif isinstance(node, ast.ClassDef):
            self._analyze_class_semantics(node, file_path, content, file_context)
        elif isinstance(node, ast.Assign):
            self._analyze_assignment_semantics(node, file_path, content, file_context)
    
    def _analyze_function_semantics(self, node: ast.FunctionDef, file_path: Path, content: str, file_context: Dict[str, Any]):
        """í•¨ìˆ˜ì˜ ì˜ë¯¸ì  ì—­í•  ë¶„ì„"""
        func_name = node.name.lower()
        
        # í•¨ìˆ˜ ë³¸ë¬¸ì—ì„œ ì£¼ìš” íŒ¨í„´ë“¤ ì¶”ì¶œ
        func_lines = content.split('\n')[node.lineno-1:node.end_lineno if hasattr(node, 'end_lineno') else node.lineno+10]
        func_content = '\n'.join(func_lines)
        
        func_info = {
            'name': node.name,
            'file': str(file_path),
            'service_type': file_context['service_type'],
            'line_start': node.lineno,
            'args': [arg.arg for arg in node.args.args],
            'content_snippet': func_content[:200] + '...' if len(func_content) > 200 else func_content,
            'semantic_role': self._determine_function_role(func_name, func_content),
            'business_domain': self._determine_business_domain(func_name, func_content),
            'data_operations': self._extract_data_operations(func_content),
            'external_calls': self._extract_external_calls(func_content)
        }
        
        # ì˜ë¯¸ì  ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
        self._categorize_function_semantically(func_info, func_content)
    
    def _determine_function_role(self, func_name: str, func_content: str) -> str:
        """í•¨ìˆ˜ì˜ ì˜ë¯¸ì  ì—­í•  ê²°ì •"""
        
        # ì¸ì¦ ê´€ë ¨
        if any(keyword in func_name for keyword in ['auth', 'login', 'logout', 'verify', 'validate', 'token', 'jwt']):
            if 'password' in func_content.lower() or 'credential' in func_content.lower():
                return 'authentication'
            elif 'token' in func_content.lower() or 'jwt' in func_content.lower():
                return 'token_management'
            else:
                return 'authorization'
        
        # ë°ì´í„° CRUD
        if any(keyword in func_name for keyword in ['create', 'insert', 'add', 'save']):
            return 'data_creation'
        elif any(keyword in func_name for keyword in ['get', 'find', 'fetch', 'retrieve', 'select', 'query']):
            return 'data_retrieval'
        elif any(keyword in func_name for keyword in ['update', 'modify', 'edit', 'change']):
            return 'data_modification'
        elif any(keyword in func_name for keyword in ['delete', 'remove', 'destroy']):
            return 'data_deletion'
        
        # ê²€ì¦ ê´€ë ¨
        if any(keyword in func_name for keyword in ['validate', 'check', 'verify', 'ensure']):
            return 'validation'
        
        # ë³€í™˜ ê´€ë ¨
        if any(keyword in func_name for keyword in ['convert', 'transform', 'format', 'serialize', 'deserialize']):
            return 'data_transformation'
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
        if any(keyword in func_name for keyword in ['process', 'handle', 'execute', 'perform']):
            return 'business_logic'
        
        # ì™¸ë¶€ ì—°ë™
        if any(keyword in func_name for keyword in ['call', 'request', 'send', 'fetch', 'client']):
            return 'external_integration'
        
        # ì„¤ì • ê´€ë ¨
        if any(keyword in func_name for keyword in ['config', 'setup', 'init', 'configure']):
            return 'configuration'
        
        return 'unknown'
    
    def _determine_business_domain(self, func_name: str, func_content: str) -> str:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ê²°ì •"""
        content_lower = func_content.lower()
        func_name_lower = func_name.lower()
        
        # ì‚¬ìš©ì ê´€ë¦¬
        if any(keyword in func_name_lower or keyword in content_lower 
               for keyword in ['user', 'account', 'profile', 'member']):
            return 'user_management'
        
        # ì¸ì¦/ë³´ì•ˆ
        if any(keyword in func_name_lower or keyword in content_lower 
               for keyword in ['auth', 'security', 'permission', 'role', 'jwt', 'token']):
            return 'authentication_security'
        
        # ê°ì‚¬/ë¡œê¹…
        if any(keyword in func_name_lower or keyword in content_lower 
               for keyword in ['audit', 'log', 'event', 'tracking']):
            return 'audit_logging'
        
        # ì˜¨í†¨ë¡œì§€/ìŠ¤í‚¤ë§ˆ
        if any(keyword in func_name_lower or keyword in content_lower 
               for keyword in ['schema', 'ontology', 'branch', 'document', 'property']):
            return 'ontology_management'
        
        # ë°ì´í„°ë² ì´ìŠ¤
        if any(keyword in func_name_lower or keyword in content_lower 
               for keyword in ['database', 'query', 'transaction', 'connection']):
            return 'database_operations'
        
        return 'general'
    
    def _extract_data_operations(self, func_content: str) -> List[str]:
        """ë°ì´í„° ì¡°ì‘ íŒ¨í„´ ì¶”ì¶œ"""
        operations = []
        content_lower = func_content.lower()
        
        # SQL íŒ¨í„´
        if re.search(r'select|insert|update|delete|create table', content_lower):
            operations.append('sql_operations')
        
        # ORM íŒ¨í„´
        if re.search(r'\.query\(|\.filter\(|\.join\(|\.add\(|\.commit\(', content_lower):
            operations.append('orm_operations')
        
        # JSON ì²˜ë¦¬
        if re.search(r'json\.|\.json\(|json\.loads|json\.dumps', content_lower):
            operations.append('json_processing')
        
        # ê²€ì¦ íŒ¨í„´
        if re.search(r'validate|check|verify|assert', content_lower):
            operations.append('validation')
        
        # ë³€í™˜ íŒ¨í„´
        if re.search(r'convert|transform|serialize|deserialize|format', content_lower):
            operations.append('transformation')
        
        return operations
    
    def _extract_external_calls(self, func_content: str) -> List[str]:
        """ì™¸ë¶€ í˜¸ì¶œ íŒ¨í„´ ì¶”ì¶œ"""
        external_calls = []
        content_lower = func_content.lower()
        
        # HTTP í˜¸ì¶œ
        if re.search(r'httpx\.|requests\.|\.get\(|\.post\(|\.put\(|\.delete\(', content_lower):
            external_calls.append('http_calls')
        
        # ë°ì´í„°ë² ì´ìŠ¤ í˜¸ì¶œ
        if re.search(r'session\.|db\.|database\.|connection\.', content_lower):
            external_calls.append('database_calls')
        
        # Redis í˜¸ì¶œ
        if re.search(r'redis\.|cache\.|\.set\(|\.get\(.*redis', content_lower):
            external_calls.append('cache_calls')
        
        # TerminusDB í˜¸ì¶œ
        if re.search(r'terminusdb|terminus', content_lower):
            external_calls.append('terminusdb_calls')
        
        return external_calls
    
    def _categorize_function_semantically(self, func_info: Dict[str, Any], func_content: str):
        """í•¨ìˆ˜ë¥¼ ì˜ë¯¸ì  ì¹´í…Œê³ ë¦¬ì— ë¶„ë¥˜"""
        role = func_info['semantic_role']
        domain = func_info['business_domain']
        
        # ì—­í• ë³„ ì¹´í…Œê³ ë¦¬
        if role == 'authentication':
            self.auth_patterns['authentication'].append(func_info)
        elif role == 'token_management':
            self.auth_patterns['token_management'].append(func_info)
        elif role == 'authorization':
            self.auth_patterns['authorization'].append(func_info)
        elif role == 'validation':
            self.validation_patterns[domain].append(func_info)
        elif role in ['data_creation', 'data_retrieval', 'data_modification', 'data_deletion']:
            self.database_patterns[role].append(func_info)
        elif role == 'data_transformation':
            self.transformation_patterns[domain].append(func_info)
        elif role == 'business_logic':
            self.business_logic_patterns[domain].append(func_info)
        elif role == 'external_integration':
            self.integration_patterns[domain].append(func_info)
        elif role == 'configuration':
            self.config_patterns[domain].append(func_info)
        
        # íŠ¹ë³„í•œ íŒ¨í„´ë“¤
        if 'error' in func_content.lower() or 'exception' in func_content.lower():
            self.error_handling_patterns[domain].append(func_info)
        
        if 'log' in func_content.lower() or 'logger' in func_content.lower():
            self.logging_patterns[domain].append(func_info)
    
    def _analyze_class_semantics(self, node: ast.ClassDef, file_path: Path, content: str, file_context: Dict[str, Any]):
        """í´ë˜ìŠ¤ì˜ ì˜ë¯¸ì  ì—­í•  ë¶„ì„"""
        class_name = node.name.lower()
        
        class_info = {
            'name': node.name,
            'file': str(file_path),
            'service_type': file_context['service_type'],
            'methods': [method.name for method in node.body if isinstance(method, ast.FunctionDef)],
            'bases': [self._get_base_name(base) for base in node.bases],
            'semantic_role': self._determine_class_role(class_name, node),
            'business_domain': self._determine_business_domain(class_name, content)
        }
        
        # í´ë˜ìŠ¤ ì—­í• ë³„ ë¶„ë¥˜
        role = class_info['semantic_role']
        if role == 'service':
            self.service_patterns[class_info['business_domain']].append(class_info)
        elif role == 'repository':
            self.repository_patterns[class_info['business_domain']].append(class_info)
        elif role == 'model':
            self.model_patterns[class_info['business_domain']].append(class_info)
        elif role == 'middleware':
            self.middleware_patterns[class_info['business_domain']].append(class_info)
    
    def _determine_class_role(self, class_name: str, node: ast.ClassDef) -> str:
        """í´ë˜ìŠ¤ì˜ ì˜ë¯¸ì  ì—­í•  ê²°ì •"""
        
        # ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
        if 'service' in class_name:
            return 'service'
        
        # ë ˆí¬ì§€í† ë¦¬ í´ë˜ìŠ¤
        if any(keyword in class_name for keyword in ['repository', 'repo', 'dao']):
            return 'repository'
        
        # ëª¨ë¸ í´ë˜ìŠ¤
        if any(keyword in class_name for keyword in ['model', 'schema', 'entity']):
            return 'model'
        
        # ë¯¸ë“¤ì›¨ì–´ í´ë˜ìŠ¤
        if 'middleware' in class_name:
            return 'middleware'
        
        # í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
        if 'client' in class_name:
            return 'client'
        
        # í•¸ë“¤ëŸ¬ í´ë˜ìŠ¤
        if 'handler' in class_name:
            return 'handler'
        
        # ë©”ì„œë“œ ì´ë¦„ìœ¼ë¡œ ì¶”ë¡ 
        methods = [method.name for method in node.body if isinstance(method, ast.FunctionDef)]
        
        # CRUD ë©”ì„œë“œê°€ ë§ìœ¼ë©´ ë ˆí¬ì§€í† ë¦¬
        crud_methods = sum(1 for method in methods if any(crud in method.lower() 
                          for crud in ['create', 'read', 'update', 'delete', 'find', 'get', 'save']))
        if crud_methods >= 2:
            return 'repository'
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”ì„œë“œê°€ ë§ìœ¼ë©´ ì„œë¹„ìŠ¤
        business_methods = sum(1 for method in methods if any(biz in method.lower() 
                              for biz in ['process', 'handle', 'execute', 'perform', 'manage']))
        if business_methods >= 2:
            return 'service'
        
        return 'unknown'
    
    def _get_base_name(self, node: ast.AST) -> str:
        """ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì´ë¦„ ì¶”ì¶œ"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_base_name(node.value)}.{node.attr}"
        return 'unknown'
    
    def _analyze_assignment_semantics(self, node: ast.Assign, file_path: Path, content: str, file_context: Dict[str, Any]):
        """ë³€ìˆ˜ í• ë‹¹ì˜ ì˜ë¯¸ì  íŒ¨í„´ ë¶„ì„"""
        # ì„¤ì • ë³€ìˆ˜ë“¤ ë¶„ì„
        for target in node.targets:
            if isinstance(target, ast.Name):
                var_name = target.id
                if self._is_config_variable(var_name):
                    config_info = {
                        'name': var_name,
                        'file': str(file_path),
                        'service_type': file_context['service_type'],
                        'config_type': self._determine_config_type(var_name)
                    }
                    self.config_patterns[config_info['config_type']].append(config_info)
    
    def _is_config_variable(self, var_name: str) -> bool:
        """ì„¤ì • ë³€ìˆ˜ì¸ì§€ í™•ì¸"""
        config_patterns = [
            'URL', 'HOST', 'PORT', 'KEY', 'SECRET', 'TOKEN', 'DATABASE', 
            'REDIS', 'CONFIG', 'SETTING', 'ENDPOINT', 'API_KEY'
        ]
        return any(pattern in var_name.upper() for pattern in config_patterns)
    
    def _determine_config_type(self, var_name: str) -> str:
        """ì„¤ì • íƒ€ì… ê²°ì •"""
        var_upper = var_name.upper()
        
        if 'DATABASE' in var_upper or 'DB' in var_upper:
            return 'database_config'
        elif 'JWT' in var_upper or 'TOKEN' in var_upper:
            return 'auth_config'
        elif 'REDIS' in var_upper:
            return 'cache_config'
        elif 'URL' in var_upper or 'ENDPOINT' in var_upper:
            return 'service_config'
        else:
            return 'general_config'
    
    def _detect_semantic_duplicates(self) -> Dict[str, Any]:
        """ì˜ë¯¸ì  ì¤‘ë³µ íƒì§€"""
        logger.info("ğŸ” ì˜ë¯¸ì  ì¤‘ë³µ íŒ¨í„´ íƒì§€ ì¤‘...")
        
        duplicates = {
            'authentication_duplicates': self._find_auth_duplicates(),
            'data_access_duplicates': self._find_data_access_duplicates(),
            'validation_duplicates': self._find_validation_duplicates(),
            'business_logic_duplicates': self._find_business_logic_duplicates(),
            'configuration_duplicates': self._find_config_duplicates(),
            'integration_duplicates': self._find_integration_duplicates(),
            'error_handling_duplicates': self._find_error_handling_duplicates(),
            'service_layer_duplicates': self._find_service_layer_duplicates(),
            'model_duplicates': self._find_model_duplicates(),
            'transformation_duplicates': self._find_transformation_duplicates()
        }
        
        return duplicates
    
    def _find_auth_duplicates(self) -> List[Dict[str, Any]]:
        """ì¸ì¦ ê´€ë ¨ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        # í† í° ê´€ë¦¬ ì¤‘ë³µ
        if len(self.auth_patterns['token_management']) > 1:
            duplicates.append({
                'type': 'token_management',
                'description': 'í† í° ê´€ë¦¬ ë¡œì§ì´ ì—¬ëŸ¬ ê³³ì—ì„œ êµ¬í˜„ë¨',
                'severity': 'high',
                'functions': self.auth_patterns['token_management'],
                'recommendation': 'arrakis-commonì˜ JWT í•¸ë“¤ëŸ¬ë¡œ í†µí•©'
            })
        
        # ì¸ì¦ ë¡œì§ ì¤‘ë³µ
        if len(self.auth_patterns['authentication']) > 1:
            services = set(func['service_type'] for func in self.auth_patterns['authentication'])
            if len(services) > 1:
                duplicates.append({
                    'type': 'authentication_logic',
                    'description': 'ì¸ì¦ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µ êµ¬í˜„ë¨',
                    'severity': 'high',
                    'functions': self.auth_patterns['authentication'],
                    'affected_services': list(services),
                    'recommendation': 'ì¤‘ì•™í™”ëœ ì¸ì¦ ì„œë¹„ìŠ¤ ë˜ëŠ” ê³µí†µ ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš©'
                })
        
        return duplicates
    
    def _find_data_access_duplicates(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° ì ‘ê·¼ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for operation_type in ['data_creation', 'data_retrieval', 'data_modification', 'data_deletion']:
            if len(self.database_patterns[operation_type]) > 1:
                # ê°™ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ì—ì„œ ì¤‘ë³µë˜ëŠ”ì§€ í™•ì¸
                domain_groups = defaultdict(list)
                for func in self.database_patterns[operation_type]:
                    domain_groups[func['business_domain']].append(func)
                
                for domain, funcs in domain_groups.items():
                    if len(funcs) > 1:
                        services = set(func['service_type'] for func in funcs)
                        if len(services) > 1:
                            duplicates.append({
                                'type': f'{operation_type}_duplication',
                                'description': f'{domain} ë„ë©”ì¸ì˜ {operation_type} ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ êµ¬í˜„ë¨',
                                'severity': 'medium',
                                'functions': funcs,
                                'business_domain': domain,
                                'affected_services': list(services),
                                'recommendation': f'{domain} ì „ìš© ë ˆí¬ì§€í† ë¦¬ ë˜ëŠ” ì„œë¹„ìŠ¤ ë ˆì´ì–´ ìƒì„±'
                            })
        
        return duplicates
    
    def _find_validation_duplicates(self) -> List[Dict[str, Any]]:
        """ê²€ì¦ ë¡œì§ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, validators in self.validation_patterns.items():
            if len(validators) > 1:
                services = set(validator['service_type'] for validator in validators)
                if len(services) > 1:
                    duplicates.append({
                        'type': 'validation_logic_duplication',
                        'description': f'{domain} ë„ë©”ì¸ì˜ ê²€ì¦ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                        'severity': 'medium',
                        'functions': validators,
                        'business_domain': domain,
                        'affected_services': list(services),
                        'recommendation': f'arrakis-commonì— {domain} ê²€ì¦ ëª¨ë“ˆ ì¶”ê°€'
                    })
        
        return duplicates
    
    def _find_business_logic_duplicates(self) -> List[Dict[str, Any]]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, logic_funcs in self.business_logic_patterns.items():
            if len(logic_funcs) > 1:
                # í•¨ìˆ˜ëª… ìœ ì‚¬ì„± ì²´í¬
                name_groups = defaultdict(list)
                for func in logic_funcs:
                    # í•¨ìˆ˜ëª…ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                    name_keywords = set(re.findall(r'[a-z]+', func['name'].lower()))
                    key = frozenset(name_keywords)
                    name_groups[key].append(func)
                
                for keyword_set, funcs in name_groups.items():
                    if len(funcs) > 1:
                        services = set(func['service_type'] for func in funcs)
                        if len(services) > 1:
                            duplicates.append({
                                'type': 'business_logic_duplication',
                                'description': f'{domain} ë„ë©”ì¸ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                                'severity': 'high',
                                'functions': funcs,
                                'business_domain': domain,
                                'similar_function_names': [func['name'] for func in funcs],
                                'affected_services': list(services),
                                'recommendation': f'{domain} ë„ë©”ì¸ ì„œë¹„ìŠ¤ ë ˆì´ì–´ í†µí•© ë˜ëŠ” ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬í™”'
                            })
        
        return duplicates
    
    def _find_config_duplicates(self) -> List[Dict[str, Any]]:
        """ì„¤ì • ê´€ë ¨ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for config_type, configs in self.config_patterns.items():
            if len(configs) > 1:
                services = set(config['service_type'] for config in configs)
                if len(services) > 1:
                    duplicates.append({
                        'type': 'configuration_duplication',
                        'description': f'{config_type} ì„¤ì •ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                        'severity': 'low',
                        'configurations': configs,
                        'config_type': config_type,
                        'affected_services': list(services),
                        'recommendation': 'arrakis-commonì— í†µí•© ì„¤ì • ê´€ë¦¬ì ìƒì„±'
                    })
        
        return duplicates
    
    def _find_integration_duplicates(self) -> List[Dict[str, Any]]:
        """ì™¸ë¶€ ì—°ë™ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, integrations in self.integration_patterns.items():
            if len(integrations) > 1:
                services = set(integ['service_type'] for integ in integrations)
                if len(services) > 1:
                    duplicates.append({
                        'type': 'external_integration_duplication',
                        'description': f'{domain} ë„ë©”ì¸ì˜ ì™¸ë¶€ ì—°ë™ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                        'severity': 'medium',
                        'functions': integrations,
                        'business_domain': domain,
                        'affected_services': list(services),
                        'recommendation': f'{domain} ì „ìš© í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„±'
                    })
        
        return duplicates
    
    def _find_error_handling_duplicates(self) -> List[Dict[str, Any]]:
        """ì—ëŸ¬ ì²˜ë¦¬ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, error_handlers in self.error_handling_patterns.items():
            if len(error_handlers) > 1:
                services = set(handler['service_type'] for handler in error_handlers)
                if len(services) > 1:
                    duplicates.append({
                        'type': 'error_handling_duplication',
                        'description': f'{domain} ë„ë©”ì¸ì˜ ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                        'severity': 'medium',
                        'functions': error_handlers,
                        'business_domain': domain,
                        'affected_services': list(services),
                        'recommendation': 'arrakis-commonì— í‘œì¤€í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ ëª¨ë“ˆ ì¶”ê°€'
                    })
        
        return duplicates
    
    def _find_service_layer_duplicates(self) -> List[Dict[str, Any]]:
        """ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, services in self.service_patterns.items():
            if len(services) > 1:
                service_types = set(service['service_type'] for service in services)
                if len(service_types) > 1:
                    duplicates.append({
                        'type': 'service_layer_duplication',
                        'description': f'{domain} ë„ë©”ì¸ì˜ ì„œë¹„ìŠ¤ ë ˆì´ì–´ê°€ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                        'severity': 'high',
                        'classes': services,
                        'business_domain': domain,
                        'affected_services': list(service_types),
                        'recommendation': f'{domain} ë„ë©”ì¸ ì „ìš© ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬ ë˜ëŠ” í†µí•©'
                    })
        
        return duplicates
    
    def _find_model_duplicates(self) -> List[Dict[str, Any]]:
        """ëª¨ë¸ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, models in self.model_patterns.items():
            if len(models) > 1:
                # ëª¨ë¸ëª… ìœ ì‚¬ì„± ì²´í¬
                model_names = [model['name'].lower() for model in models]
                name_similarities = defaultdict(list)
                
                for model in models:
                    base_name = re.sub(r'(model|schema|entity|dto)$', '', model['name'].lower())
                    name_similarities[base_name].append(model)
                
                for base_name, similar_models in name_similarities.items():
                    if len(similar_models) > 1:
                        services = set(model['service_type'] for model in similar_models)
                        if len(services) > 1:
                            duplicates.append({
                                'type': 'model_duplication',
                                'description': f'{domain} ë„ë©”ì¸ì˜ {base_name} ëª¨ë¸ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                                'severity': 'medium',
                                'classes': similar_models,
                                'business_domain': domain,
                                'base_model_name': base_name,
                                'affected_services': list(services),
                                'recommendation': f'arrakis-commonì— {base_name} ê³µí†µ ëª¨ë¸ ì •ì˜'
                            })
        
        return duplicates
    
    def _find_transformation_duplicates(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë³€í™˜ ì¤‘ë³µ ì°¾ê¸°"""
        duplicates = []
        
        for domain, transformations in self.transformation_patterns.items():
            if len(transformations) > 1:
                services = set(trans['service_type'] for trans in transformations)
                if len(services) > 1:
                    duplicates.append({
                        'type': 'data_transformation_duplication',
                        'description': f'{domain} ë„ë©”ì¸ì˜ ë°ì´í„° ë³€í™˜ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì—ì„œ ì¤‘ë³µë¨',
                        'severity': 'medium',
                        'functions': transformations,
                        'business_domain': domain,
                        'affected_services': list(services),
                        'recommendation': f'arrakis-commonì— {domain} ë°ì´í„° ë³€í™˜ ìœ í‹¸ë¦¬í‹° ì¶”ê°€'
                    })
        
        return duplicates
    
    def generate_comprehensive_report(self, semantic_duplicates: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ê´„ì  ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        total_duplicates = sum(len(category) for category in semantic_duplicates.values())
        
        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        high_severity = []
        medium_severity = []
        low_severity = []
        
        for category_name, duplicates in semantic_duplicates.items():
            for duplicate in duplicates:
                if duplicate.get('severity') == 'high':
                    high_severity.append(duplicate)
                elif duplicate.get('severity') == 'medium':
                    medium_severity.append(duplicate)
                else:
                    low_severity.append(duplicate)
        
        # ì˜í–¥ë°›ëŠ” ì„œë¹„ìŠ¤ ë¶„ì„
        affected_services = set()
        for category in semantic_duplicates.values():
            for duplicate in category:
                if 'affected_services' in duplicate:
                    affected_services.update(duplicate['affected_services'])
        
        report = {
            'analysis_timestamp': datetime.utcnow().isoformat(),
            'analysis_type': 'semantic_duplicate_detection',
            'summary': {
                'total_semantic_duplicates': total_duplicates,
                'high_severity_issues': len(high_severity),
                'medium_severity_issues': len(medium_severity),
                'low_severity_issues': len(low_severity),
                'affected_services': list(affected_services),
                'most_problematic_areas': self._identify_problematic_areas(semantic_duplicates)
            },
            'detailed_analysis': semantic_duplicates,
            'priority_recommendations': self._generate_priority_recommendations(high_severity, medium_severity),
            'architectural_insights': self._generate_architectural_insights(semantic_duplicates),
            'refactoring_roadmap': self._generate_refactoring_roadmap(semantic_duplicates)
        }
        
        return report
    
    def _identify_problematic_areas(self, semantic_duplicates: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ì˜ì—­ ì‹ë³„"""
        problem_areas = []
        
        for category_name, duplicates in semantic_duplicates.items():
            if duplicates:
                high_severity_count = sum(1 for dup in duplicates if dup.get('severity') == 'high')
                if high_severity_count > 0:
                    problem_areas.append({
                        'area': category_name,
                        'high_severity_count': high_severity_count,
                        'total_duplicates': len(duplicates)
                    })
        
        # ì‹¬ê°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        problem_areas.sort(key=lambda x: x['high_severity_count'], reverse=True)
        return problem_areas[:5]  # ìƒìœ„ 5ê°œ
    
    def _generate_priority_recommendations(self, high_severity: List, medium_severity: List) -> List[str]:
        """ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ê³ ì‹¬ê°ë„ ì´ìŠˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        auth_issues = [issue for issue in high_severity if 'auth' in issue['type']]
        if auth_issues:
            recommendations.append("ğŸ”´ [ìµœìš°ì„ ] ì¸ì¦/ê¶Œí•œ ë¡œì§ì„ arrakis-commonìœ¼ë¡œ í†µí•©í•˜ì—¬ ë³´ì•ˆ ì¼ê´€ì„± í™•ë³´")
        
        business_logic_issues = [issue for issue in high_severity if 'business_logic' in issue['type']]
        if business_logic_issues:
            recommendations.append("ğŸ”´ [ìµœìš°ì„ ] ì¤‘ë³µëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë„ë©”ì¸ë³„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬ ë˜ëŠ” í†µí•©")
        
        service_layer_issues = [issue for issue in high_severity if 'service_layer' in issue['type']]
        if service_layer_issues:
            recommendations.append("ğŸ”´ [ìµœìš°ì„ ] ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì•„í‚¤í…ì²˜ ì¬ì„¤ê³„ - ë„ë©”ì¸ ê²½ê³„ ëª…í™•í™”")
        
        # ì¤‘ê°„ì‹¬ê°ë„ ì´ìŠˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        data_issues = [issue for issue in medium_severity if 'data' in issue['type']]
        if data_issues:
            recommendations.append("ğŸŸ¡ [ì¤‘ê°„] ë°ì´í„° ì ‘ê·¼ ë ˆì´ì–´ í‘œì¤€í™” - Repository íŒ¨í„´ ì ìš©")
        
        validation_issues = [issue for issue in medium_severity if 'validation' in issue['type']]
        if validation_issues:
            recommendations.append("ğŸŸ¡ [ì¤‘ê°„] ê²€ì¦ ë¡œì§ì„ arrakis-common ìœ í‹¸ë¦¬í‹°ë¡œ í‘œì¤€í™”")
        
        return recommendations
    
    def _generate_architectural_insights(self, semantic_duplicates: Dict[str, Any]) -> List[str]:
        """ì•„í‚¤í…ì²˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì„œë¹„ìŠ¤ ê²½ê³„ ë¬¸ì œ
        service_boundary_issues = []
        for category in semantic_duplicates.values():
            for duplicate in category:
                if duplicate.get('severity') == 'high' and len(duplicate.get('affected_services', [])) > 1:
                    service_boundary_issues.append(duplicate)
        
        if service_boundary_issues:
            insights.append("âš ï¸ ì„œë¹„ìŠ¤ ê²½ê³„ê°€ ë¶ˆë¶„ëª…í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì´ ì—¬ëŸ¬ ì„œë¹„ìŠ¤ì— ë¶„ì‚°ë¨")
        
        # ê³µí†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶€ì¡±
        common_lib_needs = []
        for category in semantic_duplicates.values():
            for duplicate in category:
                if 'arrakis-common' in duplicate.get('recommendation', ''):
                    common_lib_needs.append(duplicate)
        
        if len(common_lib_needs) > 3:
            insights.append("ğŸ“š arrakis-common ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¶©ë¶„íˆ í™œìš©ë˜ì§€ ì•Šì•„ ì¤‘ë³µ ì½”ë“œ ë°œìƒ")
        
        # ë„ë©”ì¸ ë¶„ë¦¬ í•„ìš”ì„±
        domain_separation_needs = set()
        for category in semantic_duplicates.values():
            for duplicate in category:
                if duplicate.get('business_domain') and duplicate.get('business_domain') != 'general':
                    domain_separation_needs.add(duplicate['business_domain'])
        
        if len(domain_separation_needs) > 2:
            insights.append(f"ğŸ—ï¸ {', '.join(domain_separation_needs)} ë„ë©”ì¸ë“¤ì˜ ëª…í™•í•œ ë¶„ë¦¬ê°€ í•„ìš”")
        
        return insights
    
    def _generate_refactoring_roadmap(self, semantic_duplicates: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë¦¬íŒ©í† ë§ ë¡œë“œë§µ ìƒì„±"""
        roadmap = []
        
        # Phase 1: ë³´ì•ˆ ë° ì¸ì¦ í†µí•©
        auth_duplicates = semantic_duplicates.get('authentication_duplicates', [])
        if auth_duplicates:
            roadmap.append({
                'phase': 1,
                'title': 'ì¸ì¦/ë³´ì•ˆ ì‹œìŠ¤í…œ í†µí•©',
                'priority': 'high',
                'estimated_effort': 'high',
                'tasks': [
                    'arrakis-common JWT í•¸ë“¤ëŸ¬ í™•ì¥',
                    'ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ì¸ì¦ ë¯¸ë“¤ì›¨ì–´ í‘œì¤€í™”',
                    'ì¤‘ì•™í™”ëœ ê¶Œí•œ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•'
                ],
                'affected_duplicates': len(auth_duplicates)
            })
        
        # Phase 2: ë°ì´í„° ì ‘ê·¼ ë ˆì´ì–´ í‘œì¤€í™”
        data_duplicates = semantic_duplicates.get('data_access_duplicates', [])
        if data_duplicates:
            roadmap.append({
                'phase': 2,
                'title': 'ë°ì´í„° ì ‘ê·¼ ë ˆì´ì–´ í‘œì¤€í™”',
                'priority': 'medium',
                'estimated_effort': 'medium',
                'tasks': [
                    'Repository íŒ¨í„´ í‘œì¤€í™”',
                    'ê³µí†µ ë°ì´í„° ì ‘ê·¼ ì¸í„°í˜ì´ìŠ¤ ì •ì˜',
                    'ORM ì‚¬ìš© íŒ¨í„´ í†µì¼'
                ],
                'affected_duplicates': len(data_duplicates)
            })
        
        # Phase 3: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë„ë©”ì¸ë³„ ë¶„ë¦¬
        business_duplicates = semantic_duplicates.get('business_logic_duplicates', [])
        if business_duplicates:
            roadmap.append({
                'phase': 3,
                'title': 'ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë„ë©”ì¸ ë¶„ë¦¬',
                'priority': 'high',
                'estimated_effort': 'high',
                'tasks': [
                    'ë„ë©”ì¸ë³„ ì„œë¹„ìŠ¤ ê²½ê³„ ì¬ì •ì˜',
                    'ì¤‘ë³µ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í†µí•©',
                    'ë„ë©”ì¸ ì´ë²¤íŠ¸ ê¸°ë°˜ í†µì‹  êµ¬í˜„'
                ],
                'affected_duplicates': len(business_duplicates)
            })
        
        return roadmap


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = SemanticDuplicateAnalyzer("/Users/isihyeon/Desktop/Arrakis-Project")
    
    logger.info("ğŸ§  ì˜ë¯¸ì  ì¤‘ë³µ ë¶„ì„ ì‹œì‘ - Ultra Deep Analysis")
    logger.info("=" * 80)
    
    semantic_duplicates = analyzer.analyze_codebase()
    report = analyzer.generate_comprehensive_report(semantic_duplicates)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    logger.info(f"ğŸ“Š ì˜ë¯¸ì  ì¤‘ë³µ ë¶„ì„ ì™„ë£Œ")
    logger.info(f"ğŸ” ì´ {report['summary']['total_semantic_duplicates']}ê°œ ì˜ë¯¸ì  ì¤‘ë³µ ë°œê²¬")
    logger.info(f"ğŸ”´ ê³ ì‹¬ê°ë„: {report['summary']['high_severity_issues']}ê°œ")
    logger.info(f"ğŸŸ¡ ì¤‘ê°„ì‹¬ê°ë„: {report['summary']['medium_severity_issues']}ê°œ")
    logger.info(f"ğŸŸ¢ ì €ì‹¬ê°ë„: {report['summary']['low_severity_issues']}ê°œ")
    
    if report['summary']['most_problematic_areas']:
        logger.warning("âš ï¸  ê°€ì¥ ë¬¸ì œê°€ ë˜ëŠ” ì˜ì—­ë“¤:")
        for area in report['summary']['most_problematic_areas']:
            logger.warning(f"  - {area['area']}: ê³ ì‹¬ê°ë„ {area['high_severity_count']}ê°œ")
    
    if report['priority_recommendations']:
        logger.info("ğŸ’¡ ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­:")
        for rec in report['priority_recommendations']:
            logger.info(f"  {rec}")
    
    # ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"semantic_duplicate_analysis_{timestamp}.json"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ“„ ìƒì„¸ ì˜ë¯¸ì  ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {report_file}")
    
    if report['summary']['total_semantic_duplicates'] == 0:
        logger.info("ğŸ‰ ì˜ë¯¸ì  ì¤‘ë³µ ì—†ìŒ! ì•„í‚¤í…ì²˜ê°€ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        logger.warning(f"ğŸ”§ {report['summary']['total_semantic_duplicates']}ê°œ ì˜ë¯¸ì  ì¤‘ë³µ í•´ê²° í•„ìš”")
    
    return report


if __name__ == "__main__":
    main()